# INTERVIEW TARGET
1. Ml Pipelines : Kubeflow Pipelines
2. Experiment Tracking + Model Registry: Mlflow 
3. Deployment and serving : ArgoCD + Kserve
4. Monitoring and alerting : Prometheus & Grafana
5. Ci/cd Pipelines : GitHub Actions
6. Cloud : AWS S3, ECR, EKS, IAM, SageMaker, Cost optimization, Security (AWS Secrets Manager)

# Production Ready Tech Stack: 
```Kubeflow Pipelines(KFP), Mlflow, ArgoCD, Kserve, Prometheus & Grafana, GitHub Actions, AWS, Git, GitHub, DVC, Docker, Kubernetes, Python, Linux.```

<div align="center"> 
 
# CI/CD Pipeline Using GitHub Actions

</div>

### To start ci/cd we need few source files like. 
- train.py which has main training script.
- api.py
- Dataset
- requirements.txt: file where dependencies (libraries/packages) that your project needs to run
- GitHub actions workflow file i.e main.yml:  This file defines the entire ci/cd pipeline. 
- .gitignore File: To avoid pushing unnecessary, temporary, or sensitive files into the repo. 
NOTE:- This files we get from DS and ML teams.

### What we do?(Manual work)
1. Initial Setup
2. DVC Setup (Data Version Control)
3. Push Model to S3
4. Kubernetes Cluster
5. KServe Setup
6. Test KServe Inference

To Automate Test, Build, And Deploy we use ci/cd pipelines using GitHub actions.

[GitHub Actions Repo Structure](<img width="281" height="442" alt="Screenshot 2026-02-11 at 3 11 23 PM" src="https://github.com/user-attachments/assets/97f4d798-7b12-43a0-9f4e-655594f48a66"/>)

### Ci/cd Pipeline Flow:
1. Checkout code
2. Setup Python
3. Install Dependencies
4. Unit tests (pytest)  # Code Validation
5. Lint check (flake8)
6. Security Scan (Trivy)
7. Generate Dataset 
8. Train Model 
9. Configure AWS Credentials
10. Install DVC
11. Push Model to S3 via DVC
12. Login to Amazon ECR
13. Build Inference Docker Image
14. Push Docker Image to ECR.  #CI Ends Here (Artifact + Image ready)
15. Update KServe Manifest inference.yaml with new image tag #CD starts here
16. Git Commit Deployment Config
17. ArgoCD Syncs Deployment
18. KServe Deploys Model pod
19. Monitoring + Alerts  
`Note:- Trigger Kubeflow Training pipeline (optional). Big companies go instead of running training in GitHub runner it runes in cluster, it is heavy lift.`

### Continuous Integration (CI)
This job is triggered on every push, PR, workflow_dispatch to the main branch.

### Continuous Deployment (CD)
 -  ArgoCD
 -  KServe

<details>
  <summary><b>Click to expand: MLOps ci/cd Pipeline Stages</b></summary>

  ### MLOps ci/cd Pipeline
```  
on:  
  push:  
    branches: [cicd]  
  workflow_dispatch:  
  repository_dispatch:  

env:  
  AWS_REGION: us-east-1  
  S3_BUCKET: churn-model-bucket-cicd-abhi  
  ECR_REPOSITORY: churn-model-repo  
  IMAGE_TAG: ${{ github.sha }}  

jobs:  
  train-and-deploy:  
    runs-on: ubuntu-latest  

    permissions:  
      contents: write  

    steps:  

    # ‚úÖ 1. Checkout Code
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    # ‚úÖ 2. Setup Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    # ‚úÖ 3. Install Dependencies
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest flake8

    # ==================================================
    # ‚úÖ NEW STAGE ‚Äî CODE VALIDATION
    # ==================================================

    # ‚úÖ Unit Tests
    - name: Run Unit Tests
      run: pytest

    # ‚úÖ Lint Check
    - name: Run Lint Check
      run: flake8 .

    # ‚úÖ Security Scan (Trivy)
    - name: Security Scan using Trivy
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'

    # ==================================================
    # ‚úÖ ML TRAINING
    # ==================================================

    # ‚úÖ 4. Generate Dataset
    - name: Generate dataset
      run: python generate_data.py

    # ‚úÖ 5. Train Model
    - name: Train model
      run: python train.py

    # ==================================================
    # ‚úÖ MODEL VERSIONING
    # ==================================================

    # ‚úÖ 6. Configure AWS Credentials
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    # ‚úÖ 7. Install DVC
    - name: Install DVC
      run: pip install dvc[s3]

    # ‚úÖ 8. Push Model to S3 via DVC
    - name: Push model using DVC
      run: |
        dvc push
        echo "Model pushed to S3"

    # ==================================================
    # ‚úÖ CONTAINER BUILD + REGISTRY
    # ==================================================

    # ‚úÖ 9. Login to Amazon ECR
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1

    # ‚úÖ 10. Build Docker Image
    - name: Build Docker image
      run: |
        docker build -t ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }} .

    # ‚úÖ 11. Push Docker Image to ECR
    - name: Push Docker image
      run: |
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}

    # ==================================================
    # ‚úÖ GITOPS DEPLOYMENT PREP
    # ==================================================

    # ‚úÖ 12. Update KServe inference.yaml
    - name: Update inference.yaml with new image
      run: |
        NEW_IMAGE="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}"
        sed -i "s|image: .*|image: $NEW_IMAGE|g" k8s/inference.yaml

    # ‚úÖ 13. Commit Changes ‚Üí Triggers ArgoCD Sync
    - name: Commit updated inference.yaml
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add k8s/inference.yaml
        git commit -m "Update inference image to $IMAGE_TAG [skip ci]" || echo "No changes"
        git push || echo "No changes to push"

    # ==================================================
    # ‚úÖ OPTIONAL ‚Äî DEPLOYMENT NOTIFICATION / MONITORING HOOK
    # ==================================================

    - name: Deployment Notification
      run: echo "Deployment committed. ArgoCD + KServe will take over.‚Äù
    Note:- ArgoCD / KServe / Monitoring ‚Üí cannot be inside GitHub YAML (they run after commit)

    # ------------------------------------------------
    # ‚úÖ Wait for ArgoCD Sync
    # ------------------------------------------------
    - name: Wait for ArgoCD Sync
      run: |
        echo "Waiting for ArgoCD deployment..."
        sleep 60

    # ------------------------------------------------
    # ‚úÖ Health Check After Deploy
    # ------------------------------------------------
    - name: KServe Health Check
      run: |
        curl -f https://your-kserve-endpoint/health || exit 1

    # ------------------------------------------------
    # ‚úÖ Slack Notification
    # ------------------------------------------------
    - name: Slack Notification
      if: success()
      uses: slackapi/slack-github-action@v1
      with:
        payload: |
          {
            "text": "‚úÖ Churn Model Deployment Success - Image Tag: ${{ env.IMAGE_TAG }}"
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
   14.  ArgoCD
   15.  KServe
   16. Monitoring + alerting
```
   </details>

### Continuous Training(CT)
CT means automatically retraining the model when new data or drift is detected.

#### CT Triggered When
 1. Scheduled retraining time 
 2. and New Data Uploaded 
#### CT Steps Flow
1. Night 2 AM ‚Üí New production data pulled
2. Data validation
3. Train Model
4. Evaluate Model (If accuracy > threshold ‚Üí register model)
5. Push Model to DVC/S3
6. Trigger CD ‚Üí Deploy new model

#### Ex:-
```on:
  schedule:
    - cron: "0 2 * * *"    # Runs daily at 2 AM
  workflow_dispatch:       # Manual trigger also allowed
```
#### Ex:-
```on:
  repository_dispatch:
    types: [new-data-arrived]
```

## 1Ô∏è‚É£ After Model is Trained ‚Üí We Build Docker Image
### üîµ Case A ‚Äì Model Embedded Inside Image
If after training you build Docker image and include model file, then image contains:
- Base OS (python:3.9-slim)  
- Python runtime  
- All dependencies (scikit-learn, pandas, etc.)  
- Inference code (FastAPI / Flask app)  
- Model file (model.pkl)  
Example structure inside image:  
```
/app
  ‚îú‚îÄ‚îÄ app.py
  ‚îú‚îÄ‚îÄ model.pkl
  ‚îú‚îÄ‚îÄ requirements.txt
```
So this image is:  üëâ Code + Dependencies + Model file  
This means every retraining ‚Üí new Docker image build.  Used in small systems or simple deployments.  

### üü¢ Case B ‚Äì Model NOT Inside Image (Inference Image)
In modern production (like KServe + S3):  
After training:  
- Model saved in S3  
- Model registered in MLflow  
- Docker image is NOT rebuilt   

The image contains:  
- Base OS  
- Python  
- Inference server code  
- Logic to download/load model from S3  
It does NOT contain model file. KServe loads model dynamically from S3 path.  
```So retraining ‚Üí update model URI ‚Üí redeploy``` No image rebuild required.  
This is scalable and recommended.  

## Before Training starts  We Build Docker Image:   
Kubeflow does:  
üëâ Pull that image  
üëâ Create Pod  
üëâ Run training inside that container  
So this image is used as: üü¢ Training Environment Image  
It contains:  
Python  
ML libraries  
Training script  
Kubeflow uses it to execute pipeline steps.  
```
--- CI Phase (GitHub Actions) ---

1.  Code Commit (Git Push)
2.  CI Trigger (GitHub Actions)
3.  Code Checkout
4.  Install Dependencies
5.  Lint Check (flake8)
6.  Unit Tests (pytest)
7.  Data Pull & Versioning (DVC + S3)
8.  Model Training Pipeline ‚Äî KFP Run triggered by GitHub Actions (polls for completion)
        8a. Training            (KFP component)
        8b. Experiment Tracking (MLflow ‚Äî logs params, metrics, artifacts to S3)
        8c. Model Evaluation & Validation (KFP component ‚Äî gates pipeline on metric threshold)
9.  Model Registration (MLflow Registry ‚Äî model artifact stored in S3 via MLflow)
10. Model Promotion (Staging ‚Üí Production in MLflow Registry)
11. Build Inference Docker Image
12. Push Docker Image to ECR

--- CD Phase (ArgoCD + KServe) ---

13. Update KServe Manifest (inference.yaml ‚Äî new image tag + S3 model URI)
14. Git Commit & Push Deployment Config
15. ArgoCD Detects Drift & Syncs
16. KServe Deploys Model Pod (pulls image from ECR, model from S3)

--- Monitoring & Retraining Loop ---

17. Monitoring (Prometheus + Grafana ‚Äî latency, throughput, error rate)
18. Data/Concept Drift Detection (custom Prometheus metrics)
19. Alert Trigger (Prometheus Alertmanager)
20. Retraining Pipeline Trigger (webhook ‚Üí GitHub Actions ‚Üí new KFP run)
21. New Model Version Generated (loops back to step 8)
22. Redeployment (loops back to step 13)  
```

NOTE 8b: 
- When MLflow logs the model inside the KFP pipeline (step 8b), it writes directly to S3 automatically. There is no separate explicit upload action. 
- model artifact already stored in S3 via MLflow artifact store"
